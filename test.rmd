---
title: "Title of Project"
author: "Group 5 in Canvas; Student SIDs"
subtitle: "Project 2"
date: "University of Sydney | DATA1001 | September 2020"
output:
  html_document:
    fig_caption: yes
    number_sections: yes
    self_contained: yes
    theme: flatly
    toc: true
    toc_depth: 3
    toc_float: true
    code_folding: hide
---

# We first load all the libraries.
```{r, echo=F, message=F}
library(tidygeocoder)
library(tidyverse)
library(broom)
library(dplyr)
library(rafalib)
library(plotly)
```

<br>


# Executive Summary

<br>

# Full Report

## Initial Data Analysis (IDA)

### Setup
```{r}
# Setting the working directory to avoid using absolute path
setwd("~/Uni-Code/4_DATA1901_RStudio/Project2/")
getwd()
```

```{r, echo=F, message=F}
# TODO: Is there a chance where these libraries are loaded twice?
library(rvest)
library(dplyr)
library(lubridate)
library(tidyverse)
library(geosphere) # Load geosphere package for `distHaversine`
```

![uPic_2023Y-04M-10D_23H-50MIN-25S_yYTx71](https://raw.githubusercontent.com/Brandon-Lu737/Shared_Public_Pic_Hosting/master/uPic_2023Y-04M-10D_23H-50MIN-25S_yYTx71.png)

### Creating data frames `df_parramatta`: GENERAL
```{r}
# HOUSE_SCRAPPING

# adapted from https://embracingtherandom.com/r/web-scraping/rent-scraping/
house_scraping <- function(location = "2151/Parramatta/"){
  # Param: String `location`
  # Output: data_frame `df` ready to be exported as a `.csv` file
  
  # determine how many pages to scroll through 
  url <- paste0("https://www.auhouseprices.com/sold/list/NSW/", 
                location, 
                "1/?type=townhouse&ymin=0&ymax=0&bmin=0&bmax=0&pmin=0&pmax=0&sort=date&kw=") # type set to townhouse, no other filtering
  webpage <- read_html(url)
  # get the number of properties and the number of property displayed on each page 
  find_page_number <- webpage  %>%  html_nodes("h2") %>%  html_text() 
  find_page_number <- find_page_number[1]
  numbers <- as.numeric(regmatches(find_page_number, gregexpr("[0-9]+", find_page_number))[[1]])
  end_page <- ceiling(numbers[3] / numbers[2]) # number of total properties / number on page  = total number of pages
  # Initialize `df` tu `NULL`
  df <- NULL
  
  # Loop through each page
  for (thispage in c(1:end_page)){
    
    if (thispage %% 5 == 0){
      print(paste0( "Processing page ", thispage) )
    }
    
    # get website text
    url <- paste0("https://www.auhouseprices.com/sold/list/NSW/", 
                  location, 
                  thispage, 
                  "/?type=townhouse&ymin=0&ymax=0&bmin=0&bmax=0&pmin=0&pmax=0&sort=date&kw=") # type set to townhouse, no other filtering 
    webpage <- read_html(url)
    
    result <- webpage  %>%  html_nodes("li") %>%  html_text() 
    
    # end of the relevant content 
    result <-  result[ 1: grep("current", result) ]
    # remove the redundant "listed price" 
    result <-  result[ !grepl("List", result) ]
    # remove the price listed with rent
    result <-  result[ !grepl("Rent", result) ]
    
    # filter information on price and number of bedroom/bathroom/carspace
    price_bedroom  <- result[ grep("\\$", result)]
    price_bedroom <- strsplit( price_bedroom , "\\$")
    bedroom <- lapply(price_bedroom, `[`, 1)
    bedroom <- strsplit(unlist( trimws( bedroom) ) , "\\s+")
    
    price <-  lapply(price_bedroom, `[`, 2)
    price <- trimws(price)
    price <- as.numeric(gsub(",","", price ))
    
    # filter information on sold month and year
    # note sometimes the price is not listed , therefore only get the ones with the price 
    timesold  <- result[ grep("\\$", result)-1]
    timesold <-  trimws( gsub("Sold on","", timesold )) 
    
    # whether to use day month year or just month year
    timesold <- lapply(timesold , function(x){
      check_format <- strsplit(x, "\\s")
      if (length(check_format[[1]]) == 3){
        x <- dmy(x)
      }else if (length(check_format[[1]]) == 2){
        x <- my(x)
      }else{
        x <-  as.Date(paste0(x, "-01-01"))
      }
      x
    })
    timesold <- do.call("c", timesold)
    
    # get address of these properties
    address <- webpage  %>%  html_nodes("h4") %>%  html_text() 
    # end of the relevant content 
    address <-  address[ 1: grep("Auction History", address) -1 ]
    
    # decide which address contain sold price  
    sold_info <- grep("Sold on", result) # entry with sold info
    price_info <- grep("\\$", result) # entry with price info
    contain_price <- sold_info  %in% c(price_info-1) # for every sold entry, the immediate next row should be price, if not, then this sold entry does not have price record 
    address <- address[contain_price] # only record those property that has price recorded
    
    # Exporting data into a `temp_df`
    # Note that this is preliminary without any location information
    temp_df <- data.frame( address = address, 
                           bedroom = as.numeric( unlist( lapply( bedroom, `[`, 1) ) ), 
                           bathroom = as.numeric(  unlist( lapply( bedroom, `[`, 2) ) ),  
                           carspace =  as.numeric( unlist( lapply( bedroom, `[`, 3) ) ), 
                           soldprice = price,
                           yearsold =timesold )
    
    df <- rbind(df, temp_df)
  }
  
  # Return `df`, ready to be exported as a `csv` file
  return(df)
}
```

https://sydneysuburbreviews.com/inner-west-suburb-rankings/ <-- this site was used to cover all major suburbs 

### Generic function taking a list of suburbs and geolocation data:
```{r}
get_l_suburb <- function(location, suburb_lat, suburb_lon) {
  df_suburb <- house_scraping("2122/eastwood/")
  l_suburb <- df_suburb %>% geocode(address, method = 'arcgis', lat=latitude, long=longitude)
  
  l_suburb_dist <- data.frame(
    l_suburb, distance_to_train_station = apply(
      l_suburb[,c("latitude","longitude")], 1, function(x) data_distance_between(x[1], x[2], suburb_lat, suburb_lon))
  )
  
  # Writing the `l_granville_houseprice.csv` file in "~/csv_cache/"
  if (!dir.exists("~/csv_cache")) {
    dir.create("csv_cache")  # create directory if it doesn't exist
  }
  file_name <- paste0("l_", gsub("/", "", location), "_houseprice.csv")
  print(file_name)
  file_path <- file.path("csv_cache", file_name)  # specify file path
  write.csv(l_suburb_dist, file_path, row.names = FALSE)  # export as CSV file
  print("Finished")
}
get_l_suburb("2122/eastwood/", -33.7899, 151.0821)
```

```{r}
# file_name <- paste0("l_", location, ".csv")
# # [1] "Processing page 5"
# # [1] "Processing page 10"
# # [1] "Processing page 15"
# # Passing 183 addresses to the ArcGIS single address geocoder
# # Query completed in: 104.1 seconds
# # Warning: 'csv_cache' already existsWarning: cannot open file 'csv_cache/l_2122/eastwood/.csv': No such file or directoryError in file(file, ifelse(append, "a", "w")) : 
# #   cannot open the connection
```


### Creating data frames `df_parramatta`: SPECIFIC
```{r}
# TODO: Combining all these different locations into one single function
# TODO: For the function, input a list of `int[]` and output the corresponding results
# suburb name with space need to be joined with "+" sign 
df_parramatta <- house_scraping(location = "2150/parramatta/")
df_merrylands <- house_scraping(location = "2160/merrylands/")
df_auburn <- house_scraping(location = "2144/auburn/")
df_eastwood <- house_scraping(location = "2122/eastwood/")
df_granville <- house_scraping(location = "2142/granville/")
```


### A geolocation: `l_parramatta`: SPECIFIC
```{r, eval=FALSE}
print("A geolocation: `l_parramatta`: SPECIFIC")
l_parramatta <- df_parramatta %>% geocode(address, method = 'arcgis', lat=latitude, long=longitude)
l_merrylands <- df_merrylands %>% geocode(address, method = 'arcgis', lat=latitude, long=longitude)
l_auburn <- df_auburn %>% geocode(address, method = 'arcgis', lat=latitude, long=longitude)
l_eastwood <- df_eastwood %>% geocode(address, method = 'arcgis', lat=latitude, long=longitude)
l_granville <- df_granville %>% geocode(address, method = 'arcgis', lat=latitude, long=longitude)
# Error msg: FOR WRITE.CSV
# Error in file(file, ifelse(append, "a", "w")) :
# cannot open the connection

# [1] "A geolocation: `l_parramatta`: SPECIFIC"
# Passing 354 addresses to the ArcGIS single address geocoder
# Query completed in: 183 seconds
# Passing 407 addresses to the ArcGIS single address geocoder
# Query completed in: 218.4 seconds
# Passing 577 addresses to the ArcGIS single address geocoder
# Query completed in: 311.7 seconds
# Passing 183 addresses to the ArcGIS single address geocoder
# Query completed in: 106.5 seconds
# Passing 280 addresses to the ArcGIS single address geocoder
# Query completed in: 143.5 seconds
```


### Function to calculate distance between two points using longitude and latitude from dataframe
```{r}
# function that returns the distance between places within the dataframe using longitude and latitude column, and a fixed location. Now, we just need to substitute fixed_lat and fixed_lon with the desired locations for train stations etc to incorporate into the data
data_distance_between <- function(lat, lon, fixed_lat, fixed_lon) {
  dist <- distHaversine(c(lon, lat), c(fixed_lon, fixed_lat))
  return(dist)
}  

# used Google maps for all longitudes and latitudes

# 1. Input the Google map data into the `data_distance_between()`
# 2. expport the distance and the `l_paramatta` into a new `l_paramatta_dist`
parramatta_lat <- -33.8175
parramatta_lon <- 151.0050
l_parramatta_dist <- data.frame(l_parramatta, distance_to_train_station = apply(l_parramatta[,c("latitude","longitude")], 1, function(x) data_distance_between(x[1], x[2], parramatta_lat, parramatta_lon)))

merrylands_lat <- -33.8363
merrylands_lon <- 150.9926
l_merrylands_dist <- data.frame(l_merrylands, distance_to_train_station = apply(l_merrylands[,c("latitude","longitude")], 1, function(x) data_distance_between(x[1], x[2], merrylands_lat, merrylands_lon)))

auburn_lat <- -33.8490
auburn_lon <- 151.0329
l_auburn_dist <- data.frame(l_auburn, distance_to_train_station = apply(l_auburn[,c("latitude","longitude")], 1, function(x) data_distance_between(x[1], x[2], auburn_lat, auburn_lon)))

eastwood_lat <- -33.7899
eastwood_lon <- 151.0821
l_eastwood_dist <- data.frame(l_eastwood, distance_to_train_station = apply(l_eastwood[,c("latitude","longitude")], 1, function(x) data_distance_between(x[1], x[2], eastwood_lat, eastwood_lon)))

granville_lat <- -33.8326
granville_lon <- 151.0120
l_granville_dist <- data.frame(l_granville, distance_to_train_station = apply(l_granville[,c("latitude","longitude")], 1, function(x) data_distance_between(x[1], x[2], granville_lat, granville_lon)))

# c("latitude", "longitude") takes only the latitude and longitude so x[1], x[2] takes 1st and 2nd element of the row x which is latitude and longitude 

# The apply function applies the data_distance_between function to each row of the l_chatswood dataframe. The apply function takes three arguments: the dataframe subsetted to the latitude and longitude columns (using l_chatswood[, c("latitude", "longitude")]), the 1 argument to apply the function to each row, and a function that takes the latitude and longitude values of a row as input and calculates the distance to the fixed point.

# x is a row in the dataframe. x[1] is first element of x which is longitude
```

### Exporting into `csv` file
```{r}
# Writing the `l_granville_houseprice.csv` file in "~/csv_cache/"
dir.create("csv_cache")  # create directory if it doesn't exist
file_path <- file.path("csv_cache", "l_granville_houseprice.csv")  # specify file path
write.csv(l_granville_dist, file_path, row.names = FALSE)  # export as CSV file
```


### Graphing 
```{r}
ggplot(l_parramatta_dist, aes(x=distance_to_train_station, y=soldprice))+
  geom_point(aes(color=bedroom))+
  scale_colour_gradient(low="pink")+
  labs(title = "Sale Price of Townhouses vs Distance to Train Station", x="Distance to Local Train Station (m)", y="Sale Price ($)")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme_linedraw()

ggplot(l_merrylands_dist, aes(x=distance_to_train_station, y=soldprice))+
  geom_point(aes(color=bedroom))+
  scale_colour_gradient(low="red")+
  labs(title = "Sale Price of Townhouses vs Distance to Train Station", x="Distance to Local Train Station (m)", y="Sale Price ($)")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme_linedraw()

ggplot(l_auburn_dist, aes(x=distance_to_train_station, y=soldprice))+
  geom_point(aes(color=bedroom))+
  scale_colour_gradient(low="indianred")+
  labs(title = "Sale Price of Townhouses vs Distance to Train Station", x="Distance to Local Train Station (m)", y="Sale Price ($)")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme_linedraw()

ggplot(l_eastwood_dist, aes(x=distance_to_train_station, y=soldprice))+
  geom_point(aes(color=bedroom))+
  scale_colour_gradient(low="blue")+
  labs(title = "Sale Price of Townhouses vs Distance to Train Station", x="Distance to Local Train Station (m)", y="Sale Price ($)")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme_linedraw()

ggplot(l_granville_dist, aes(x=distance_to_train_station, y=soldprice))+
  geom_point(aes(color=bedroom))+
  scale_colour_gradient(low="black")+
  labs(title = "Sale Price of Townhouses vs Distance to Train Station", x="Distance to Local Train Station (m)", y="Sale Price ($)")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme_linedraw()
```

<br>

## Research Questions 

<br>

## Related Articles

<br>

## References
Use APA

<br>

## Acknowledgments
When did you team meet (date and time), and what did each team member contribute?

<br>

## Appendix (Optional)








